# SageMaker Builder Skills â€“ Stage 2 (Titanic) | Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 (ØªÙŠØªØ§Ù†ÙŠÙƒ)

## ğŸ‡¸ğŸ‡¦ Ù…Ø§Ø°Ø§ Ø£Ù†Ø¬Ø²ØªØŸ
Ù‚Ù…Øª Ø¨Ø¨Ù†Ø§Ø¡ ØªØ¯ÙÙ‚ Ø¹Ù…Ù„ ML Ø¹Ù…Ù„ÙŠ Ø¯Ø§Ø®Ù„ **Amazon SageMaker Unified Studio** ÙŠØ´Ù…Ù„:
- ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Titanic (CSV)
- ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (ØªØ¹ÙˆÙŠØ¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ø§Ù‚ØµØ© + One-Hot Encoding)
- ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Logistic Regression Ù…Ø­Ù„ÙŠÙ‹Ø§ (scikit-learn)
- Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙƒÙ€ **Model Artifact** ÙˆØ±ÙØ¹Ù‡ Ø¥Ù„Ù‰ **Amazon S3**
- ØªØ¬Ù‡ÙŠØ² Artifact Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ SageMaker (`model.tar.gz`)
- Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† `SKLearnModel` Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø§Ø³ØªØ¶Ø§ÙØ© ÙˆØ§Ù„Ù†Ø´Ø±

### Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ù‡Ù…Ø©
- `src/train.py`: Ø³ÙƒØ±Ø¨Øª ØªØ¯Ø±ÙŠØ¨ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ SageMaker Training Jobs (Channels: train_x, train_y, test_x, test_y)
- `src/inference.py`: Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Inference) Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ CSV
- `model.tar.gz`: Model Artifact (ØªÙ… Ø±ÙØ¹Ù‡ Ø¥Ù„Ù‰ S3)

### Ù…Ù„Ø§Ø­Ø¸Ø§Øª/Ø¯Ø±ÙˆØ³ Ù…Ø³ØªÙØ§Ø¯Ø©
- Ø£Ø³Ù…Ø§Ø¡ Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ SageMaker Ù„Ø§ ØªÙ‚Ø¨Ù„ `/` (Ø§Ø³ØªØ®Ø¯Ù… `train_x` Ø¨Ø¯Ù„ `train/x`).
- Ø£Ø®Ø·Ø§Ø¡ 500 ÙÙŠ Endpoint ØºØ§Ù„Ø¨Ù‹Ø§ Ø³Ø¨Ø¨Ù‡Ø§ parsing ÙÙŠ `input_fn` Ø£Ùˆ Ø´ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (1D vs 2D).
- Ø­Ø°Ù Ø§Ù„Ù€ Endpoints Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¨Ø§Ø´Ø±Ø© Ø¶Ø±ÙˆØ±ÙŠ Ù„ØªØ¬Ù†Ø¨ Ø£ÙŠ ØªÙƒÙ„ÙØ© Ù…Ø³ØªÙ…Ø±Ø©.

### Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©
- Batch Transform (Ø¨Ø¯ÙˆÙ† Endpoint Ø¯Ø§Ø¦Ù…)
- Hyperparameter Tuning
- ØªÙˆØ«ÙŠÙ‚ MLOps (Model Registry + Pipelines)

---

## ğŸ‡ºğŸ‡¸ English â€” What I built
An end-to-end ML workflow in **Amazon SageMaker Unified Studio**:
- Loaded the Titanic dataset (CSV)
- Preprocessed data (missing values + one-hot encoding)
- Trained a Logistic Regression model locally (scikit-learn)
- Saved model artifacts and uploaded them to **Amazon S3**
- Packaged a SageMaker-compatible artifact (`model.tar.gz`)
- Created a `SKLearnModel` object for hosting/deployment experiments

### Key artifacts
- `src/train.py`: training script for SageMaker training jobs (channels: train_x, train_y, test_x, test_y)
- `src/inference.py`: inference handler for CSV payloads

### Lessons learned
- Training channel names cannot contain `/` (use `train_x` not `train/x`).
- Endpoint 500 errors typically come from input parsing / payload shape mismatches.
- Always delete endpoints after testing to avoid ongoing charges.

### Next steps
- Batch Transform inference (no persistent endpoint)
- Hyperparameter tuning
- MLOps: Model Registry + Pipelines
